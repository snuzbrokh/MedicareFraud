{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/provData/x_train.csv').set_index('Provider')\n",
    "#data_test = pd.read_csv('./data/provData/x_test_inout.csv').set_index('Provider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = ['Unnamed: 0','Unnamed: 0.1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                            0\n",
       "Gender                         0\n",
       "BeneID                         0\n",
       "ClaimID                        0\n",
       "NumDiag                        0\n",
       "OPAnnualReimbursementAmt       0\n",
       "IPAnnualReimbursementAmt       0\n",
       "TotalClaim                     0\n",
       "InscCovPercent                12\n",
       "DailyCharge                    0\n",
       "DupRecord                      0\n",
       "WhetherDead                    0\n",
       "Alzheimer                      0\n",
       "HeartFailure                   0\n",
       "KidneyDisease                  0\n",
       "Cancer                         0\n",
       "ObstrPulmonary                 0\n",
       "Depression                     0\n",
       "Diabetes                       0\n",
       "IschemicHeart                  0\n",
       "Osteoporasis                   0\n",
       "RheumatoidArthritis            0\n",
       "Stroke                         0\n",
       "LargeClaims                    0\n",
       "AdmissionDays_in               0\n",
       "ClaimDays_in                   0\n",
       "ClaimDays_out                  0\n",
       "NumChronics_in                 0\n",
       "NumChronics_out                0\n",
       "NumProc_in                     0\n",
       "NumProc_out                    0\n",
       "State_in                       0\n",
       "State_out                      0\n",
       "NumProc_Range                  0\n",
       "ClaimDays_Range                0\n",
       "TotalClaim_Range               0\n",
       "InscCovPercent_Range         168\n",
       "DailyCharge_Range              0\n",
       "PotentialFraud                 0\n",
       "docDegMax                      0\n",
       "docBtwnMean                    0\n",
       "docEignMean                    0\n",
       "docMANN                     1482\n",
       "patDegMax                      0\n",
       "patBtwnMean                    0\n",
       "patEignMean                    0\n",
       "patMANN                       18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['LargeClaims'] = data['ClaimID'] > 1300\n",
    "\n",
    "# data['LargeClaims'].value_counts()\n",
    "\n",
    "# #data.columns\n",
    "\n",
    "# smallclaims = data[data.LargeClaims == 0]\n",
    "# largeclaims = data[data.LargeClaims == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BeneID', 'ClaimID', 'NumDiag', 'OPAnnualReimbursementAmt',\n",
       "       'IPAnnualReimbursementAmt', 'TotalClaim', 'InscCovPercent',\n",
       "       'DailyCharge', 'DupRecord', 'WhetherDead', 'KidneyDisease',\n",
       "       'IschemicHeart', 'AdmissionDays_in', 'ClaimDays_in', 'ClaimDays_out',\n",
       "       'NumChronics_in', 'NumChronics_out', 'NumProc_in', 'NumProc_out',\n",
       "       'State_out', 'NumProc_Range', 'ClaimDays_Range', 'TotalClaim_Range',\n",
       "       'InscCovPercent_Range', 'DailyCharge_Range', 'docDegMax', 'docEignMean',\n",
       "       'patDegMax', 'patMANN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_unscaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['PotentialFraud']\n",
    "x_unscaled = data.drop(columns = ['Age','State_in','Gender','AdmissionDays_in','docBtwnMean','patEignMean',\n",
    "                                  'Alzheimer', 'HeartFailure','Cancer', 'ObstrPulmonary', \n",
    "                                  'Depression', 'Diabetes','Osteoporasis','patBtwnMean',\n",
    "                                  'RheumatoidArthritis', 'Stroke','docMANN','LargeClaims',\n",
    "                                  'PotentialFraud'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "x = preprocessing.scale(x_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: (5410, 28), (5410,)\n",
      "Training: (3606, 28), (3606,)\n",
      "Test:     (1804, 28), (1804,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = tts(x,y,test_size=1.0/3, random_state=0)\n",
    "\n",
    "print('Original: {}, {}'.format(x.shape, y.shape))\n",
    "print('Training: {}, {}'.format(x_train.shape, y_train.shape))\n",
    "print('Test:     {}, {}'.format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "\n",
    "def ClassMetrics(x,y, model):\n",
    "    logit_tr_acc = model.score(x, y)\n",
    "    logit_tr_pr, logit_tr_re, logit_tr_f1, _ = precision_recall_fscore_support(y, logit.predict(x))\n",
    "\n",
    "    print(\" Logit Train Accuracy : %1.3f\" % (logit_tr_acc))\n",
    "    print(\" Logit Train Precision: %1.3f (no fraud) and %1.3f (fraud)\" % (logit_tr_pr[0], logit_tr_pr[1]))\n",
    "    print(\" Logit Train Recall   : %1.3f (no fraud) and %1.3f (fraud)\" % (logit_tr_re[0], logit_tr_re[1]))\n",
    "    print(\" Logit Train F1 Score : %1.3f (no fraud) and %1.3f (fraud)\" % (logit_tr_f1[0], logit_tr_f1[1]))\n",
    "    \n",
    "    print(confusion_matrix(y,model.predict(x)))\n",
    "    #print(classification_report(y,model.predict(x)))\n",
    "    \n",
    "    y_probs_logit = pd.DataFrame(model.predict_proba(x))[1]\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_probs_logit)\n",
    "    auc = roc_auc_score(y, y_probs_logit)  # Computes auc\n",
    "    \n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw,\n",
    "            label='ROC logit (area = %0.2f)' % auc)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0, 1.02])\n",
    "    plt.ylim([0, 1.02])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0]*13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3606, 28)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(dtype=tf.float32, shape=[None,28])\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None,1])\n",
    "W = tf.Variable(initial_value=tf.random_uniform(shape=[28, 2]), dtype=tf.float32)\n",
    "b = tf.Variable(initial_value = [0,0], dtype=tf.float32)\n",
    "z = tf.nn.softmax(tf.matmul(X,W)+b)\n",
    "\n",
    "loss = tf.reduce_mean(-Y*tf.log(z))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = opt.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope coefficients:\n",
      "[[0.41811034 0.3830196 ]\n",
      " [0.7864691  0.80813915]\n",
      " [0.61021686 0.7648383 ]\n",
      " [0.53559387 0.5500573 ]\n",
      " [0.39319783 0.4151778 ]\n",
      " [0.65331316 0.6556074 ]\n",
      " [0.2914189  0.34058025]\n",
      " [0.8089391  0.6738984 ]\n",
      " [0.44612488 0.4465655 ]\n",
      " [0.4115275  0.4133639 ]\n",
      " [0.6530273  0.5562443 ]\n",
      " [0.7544298  0.7771717 ]\n",
      " [0.42977956 0.2778    ]\n",
      " [0.64344656 0.6207447 ]\n",
      " [0.6199111  0.6679561 ]\n",
      " [0.6345769  0.64262384]\n",
      " [0.6880743  0.8091498 ]\n",
      " [0.40227324 0.40313777]\n",
      " [0.522757   0.52921855]\n",
      " [0.42186573 0.36253944]\n",
      " [0.3683273  0.4066942 ]\n",
      " [0.3360954  0.333374  ]\n",
      " [0.6033198  0.5801901 ]\n",
      " [0.94675773 0.96996206]\n",
      " [0.83530146 0.83282214]\n",
      " [0.71734744 0.71484333]\n",
      " [0.3396541  0.34153897]\n",
      " [0.21585368 0.22019447]]\n",
      "intercept term:\n",
      "[-0.02666036  0.02666046]\n",
      "my prediction:\n",
      "[1 1 1 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 30000\n",
    "init = tf.global_variables_initializer()\n",
    "feed_dict = {X:x_train, Y:y_train[:,np.newaxis]}\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(num_iterations):\n",
    "        sess.run(train_op, feed_dict = feed_dict)\n",
    "    print('slope coefficients:')\n",
    "    print(sess.run(W))\n",
    "    print('intercept term:')\n",
    "    print(sess.run(b))\n",
    "    prob = sess.run(z, feed_dict=feed_dict)\n",
    "    myTarget = np.argmax(prob, axis=1)\n",
    "    print('my prediction:')\n",
    "    print(myTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 834,  146],\n",
       "       [2435,  191]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(myTarget,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logit Train Precision: 0.851 (no fraud) and 0.073 (fraud)\n",
      " Logit Train Recall   : 0.255 (no fraud) and 0.567 (fraud)\n",
      " Logit Train F1 Score : 0.393 (no fraud) and 0.129 (fraud)\n"
     ]
    }
   ],
   "source": [
    "logit_tr_pr, logit_tr_re, logit_tr_f1, _ = precision_recall_fscore_support(y_train, myTarget)\n",
    "\n",
    "print(\" Logit Train Precision: %1.3f (no fraud) and %1.3f (fraud)\" % (logit_tr_pr[0], logit_tr_pr[1]))\n",
    "print(\" Logit Train Recall   : %1.3f (no fraud) and %1.3f (fraud)\" % (logit_tr_re[0], logit_tr_re[1]))\n",
    "print(\" Logit Train F1 Score : %1.3f (no fraud) and %1.3f (fraud)\" % (logit_tr_f1[0], logit_tr_f1[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horrible Performance (no hidden layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.variable_scope('multinomial_model', reuse=tf.AUTO_REUSE) as linear_model:\n",
    "    # Setup the placeholder to input features and labels\n",
    "    X = tf.placeholder(shape=[None,28], dtype=tf.float32, name='X')\n",
    "    Y = tf.placeholder(shape=[None,1], dtype=tf.float32, name='Y')\n",
    "    \n",
    "    # define prediction using the dense layer and then define our loss\n",
    "    y = tf.layers.dense(X,units=2, activation=tf.nn.softmax, name='multinomial')\n",
    "    loss = tf.reduce_mean(-Y*tf.log(y))\n",
    "    \n",
    "    # specify the optimizer\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "    \n",
    "    # pull out the coefficients from the dense layer\n",
    "    weights = tf.get_variable('multinomial/weights', shape=[28,1])\n",
    "    bias = tf.get_variable('multinomial/bias', shape=[2,])\n",
    "    \n",
    "    # define the training op, and compute the confusion matrix\n",
    "    train_op = opt.minimize(loss) # define the training operation\n",
    "    tgt = tf.argmax(y,axis=1) # convert probabilites to integer label\n",
    "    cm = tf.confusion_matrix(tgt, y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights are:\n",
      "[[-0.2405416 ]\n",
      " [ 0.23900354]\n",
      " [ 0.08518237]\n",
      " [-0.05285555]\n",
      " [-0.32173648]\n",
      " [-0.25893193]\n",
      " [-0.36629674]\n",
      " [ 0.42501968]\n",
      " [ 0.34311426]\n",
      " [-0.1772885 ]\n",
      " [-0.14850467]\n",
      " [-0.06506535]\n",
      " [-0.04815006]\n",
      " [-0.4219667 ]\n",
      " [ 0.3438874 ]\n",
      " [-0.3124784 ]\n",
      " [ 0.12044287]\n",
      " [-0.12241527]\n",
      " [-0.32314837]\n",
      " [ 0.35406947]\n",
      " [ 0.3932485 ]\n",
      " [ 0.07562655]\n",
      " [ 0.00398391]\n",
      " [-0.43596426]\n",
      " [ 0.2920916 ]\n",
      " [-0.1482651 ]\n",
      " [ 0.35183537]\n",
      " [ 0.37251854]]\n",
      "\n",
      "The biases are:\n",
      "[ 0.00727385 -0.00727373]\n",
      "\n",
      "Counting Predicted Classes\n",
      "0    2156\n",
      "1    1450\n",
      "dtype: int64\n",
      "\n",
      "Confusion matrix:\n",
      "[[1996  160]\n",
      " [1273  177]]\n"
     ]
    }
   ],
   "source": [
    "num_iterations = 10000\n",
    "init = tf.global_variables_initializer()\n",
    "feed_dict = {X:x_train, Y:y_train[:,np.newaxis]}\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for i in range(num_iterations):\n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "    print(\"The weights are:\")        \n",
    "    print(sess.run(weights))\n",
    "    print(\"\\nThe biases are:\")\n",
    "    print(sess.run(bias))\n",
    "    tgt2 = np.argmax(sess.run(y,feed_dict=feed_dict), axis=1)\n",
    "    print('\\nCounting Predicted Classes')\n",
    "    print(pd.Series(tgt2).value_counts())\n",
    "    print('\\nConfusion matrix:')\n",
    "    print(sess.run(cm, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (12,10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
